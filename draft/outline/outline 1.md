# Reverse-prompt engineering to surface tacit human knowledge

Framing problem:

Good LLM performance--i.e. an experience the human agent enjoys that makes them more productive at
their task--requires good prompting, which requires both domain knowledge and a persistent, creative,
experimental approach. It doesn’t generalize and most people can’t do it.
The main ways to improve LLM performance in a sub-domain--pre-prompting and fine-tuning--may improve
satisfaction of users who need help with well-known problems but they may stretch our criteria for “good performance”. They surprise us less. And if LLMs can’t surprise us, are they really more exciting than the previous generation of AIs?

Breakthrough:

What if instead of focusing on LLM-driven tools that produce knowledge, we pivoted to LLM-driven tools
that surface the tacit knowledge of the human agent interacting with them?

---

> (1) properly prompting an llm is not just a specialized skill, but one that also requires domain knowledge of the end product and technical terms (e.g that a story has a plot, parameters for graphics), therefore unusable for the general public. (2) models often interpolate and are not always able to create new and/or unique versions of well established art forms e.g. creative writing, (3) getting the LLM to react creatively requires creativity of the user which the current interfaces do not engender in many users (I can provide some examples of this)
>
> Ireti