# Introduction

When applying the capabilities of Large Language Models to live applications, one faces challenges on two sides. On one side, the general scope of an LLM’s training—its fundamental strength—also hinders its ability to efficiently answer questions in a specific domain. Pre-prompting is a cheap but often insufficient approach to address this; fine-tuning is more promising, but expensive, with the required CPU cycles and training set size both increasing proportionally to the square(?) of the size of the LLM (citation?). “Solving for context” is a problem that will therefore likely continue to require a tradeoff between efficiency of deployment and quality of interactions.
On the other side, the task of narrowing an LLM’s responses to a specific domain can lead to undesired overfitting. The more specific the context for which the LLM has been “solved”, the more likely it is to provide completions that a seasoned human agent in the domain would recognize as banal, prevaricating, or unhelpful. This is not merely an artifact of the training data, but an inherent consequence of the mechanics of fine-tuning where the ability to enrich an interaction with cross-domain inferences is selectively tuned out. This second challenge we call the “porpoise problem” of creativity.
In response to this twofold issue, we propose a new paradigm of LLM interaction: reverse prompting. The core axiom of reverse prompting is: when the goal of the interaction is creative production rather than reproduction of known solutions, the human agent is in a better position to synthesize new information—or, recall information not yet included in the LLM’s training set. Therefore, in such scenarios of creative production, LLM-driven interfaces are more effective when they aim to ask thought-provoking questions of the human in the loop, rather than the other way around.
As we show, for the problems to which reverse-prompting systems are best suited, LLMs prove more efficient and effective in both of the above respects: they can produce completions that are more responsive to context with less fine-tuning (“solving for context”), and they can produce completions that bring creativity and surprise while remaining relevant (“porpoise problem”).
